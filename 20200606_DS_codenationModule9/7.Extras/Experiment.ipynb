{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from importlib.machinery import SourceFileLoader\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "DI = SourceFileLoader(\"DI\", \"src/DI.py\").load_module()\n",
    "\n",
    "Handler = DI.Handler.Handler()\n",
    "Graph = DI.Graph.Graph()\n",
    "AlgoML = DI.AlgoML.AlgoML()\n",
    "Refine = DI.Refine.Refine()\n",
    "Metrics = DI.Metrics.Metrics()\n",
    "\n",
    "_data_train = pd.read_csv(\"/home/agl/codenation/20200606_DS_codenationModule9/1.Original/train.csv\")\n",
    "_data_test = pd.read_csv(\"/home/agl/codenation/20200606_DS_codenationModule9/1.Original/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All analyzes made on file analyze.ipynb\n",
    "\n",
    "#1 - Drop the necessaries columns, but remimber storage them in a variable because will need to concat at end\n",
    "#2 - Remove the columns that have 60% null if they had low coeffience\n",
    "#3 - Create differents dataset with differentes strategy to use in model and see which will give better result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'NU_IDADE',\n",
    "    'TP_ST_CONCLUSAO',\n",
    "    'TP_ANO_CONCLUIU',\n",
    "    'TP_ESCOLA',\n",
    "]\n",
    "\n",
    "target = \"IN_TREINEIRO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get colums of inscribeds of data test and save\n",
    "inscribeds_column_all = _data_test[\"NU_INSCRICAO\"]\n",
    "\n",
    "#data test cleaned\n",
    "test = _data_test[features + [\"NU_INSCRICAO\"]].dropna()\n",
    "\n",
    "#get colums of inscribeds of data test without null and save\n",
    "inscribeds_column_cleaned = test[\"NU_INSCRICAO\"]\n",
    "\n",
    "#Remove NU_INSCRICAO of data test because not is more necessary and must to have only the features for our model\n",
    "test = test.drop(columns=\"NU_INSCRICAO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get our data train and separate it\n",
    "train = _data_train[features + [target]].dropna()\n",
    "train_target = train[target]\n",
    "train.drop(columns=target, inplace=True)\n",
    "\n",
    "#obj with x_train, y_train, x_test, y_test randomized\n",
    "obj_train = Handler.create_test_from_train(train, train_target, 0.517, 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standard data for train of obj_train\n",
    "scaler = Refine.standardize(obj_train[\"x_train\"])\n",
    "\n",
    "#Transform the indepedents variable into obj_train\n",
    "obj_train[\"x_train\"] = scaler.transform(obj_train[\"x_train\"])\n",
    "obj_train[\"x_test\"] = scaler.transform(obj_train[\"x_test\"])\n",
    "\n",
    "#Transform also our official test data\n",
    "test = scaler.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      6147\n",
      "           1       1.00      0.98      0.99       952\n",
      "\n",
      "    accuracy                           1.00      7099\n",
      "   macro avg       1.00      0.99      0.99      7099\n",
      "weighted avg       1.00      1.00      1.00      7099\n",
      "\n",
      "\n",
      "        ----------------------------------------------------------------------\n",
      "          True Negatives = 6147             |    False Positives = 0\n",
      "          False Negatives = 23            |    True Positives = 929\n",
      "        ----------------------------------------------------------------------\n",
      "            \n",
      "roc_auc_score = 0.9879201680672269\n",
      "avg_precision_score = 0.9790802290771218\n"
     ]
    }
   ],
   "source": [
    "#Result metrics\n",
    "ml = AlgoML.neighborsClassifier(obj_data=obj_train)\n",
    "\n",
    "Metrics.calculate_classification(obj_train[\"y_test\"], ml.predict(obj_train[\"x_test\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      6147\n",
      "           1       1.00      0.98      0.99       952\n",
      "\n",
      "    accuracy                           1.00      7099\n",
      "   macro avg       1.00      0.99      0.99      7099\n",
      "weighted avg       1.00      1.00      1.00      7099\n",
      "\n",
      "\n",
      "        ----------------------------------------------------------------------\n",
      "          True Negatives = 6147             |    False Positives = 0\n",
      "          False Negatives = 23            |    True Positives = 929\n",
      "        ----------------------------------------------------------------------\n",
      "            \n",
      "roc_auc_score = 0.9879201680672269\n",
      "avg_precision_score = 0.9790802290771218\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.996760107057332"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest = AlgoML.randomForestClassifier(obj_data=obj_train)\n",
    "\n",
    "Metrics.calculate_classification(obj_train[\"y_test\"], forest.predict(obj_train[\"x_test\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.96      0.98      6147\n",
      "           1       0.81      1.00      0.89       952\n",
      "\n",
      "    accuracy                           0.97      7099\n",
      "   macro avg       0.90      0.98      0.94      7099\n",
      "weighted avg       0.97      0.97      0.97      7099\n",
      "\n",
      "\n",
      "        ----------------------------------------------------------------------\n",
      "          True Negatives = 5921             |    False Positives = 226\n",
      "          False Negatives = 0            |    True Positives = 952\n",
      "        ----------------------------------------------------------------------\n",
      "            \n",
      "roc_auc_score = 0.9816170489669758\n",
      "avg_precision_score = 0.8081494057724957\n"
     ]
    }
   ],
   "source": [
    "naive = AlgoML.guassianNB(obj_data=obj_train)\n",
    "\n",
    "Metrics.calculate_classification(obj_train[\"y_test\"], naive.predict(obj_train[\"x_test\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99      6147\n",
      "           1       0.90      1.00      0.95       952\n",
      "\n",
      "    accuracy                           0.98      7099\n",
      "   macro avg       0.95      0.99      0.97      7099\n",
      "weighted avg       0.99      0.98      0.98      7099\n",
      "\n",
      "\n",
      "        ----------------------------------------------------------------------\n",
      "          True Negatives = 6037             |    False Positives = 110\n",
      "          False Negatives = 0            |    True Positives = 952\n",
      "        ----------------------------------------------------------------------\n",
      "            \n",
      "roc_auc_score = 0.9910525459573776\n",
      "avg_precision_score = 0.896421845574388\n"
     ]
    }
   ],
   "source": [
    "svm = AlgoML.svm(obj_data=obj_train)\n",
    "\n",
    "Metrics.calculate_classification(obj_train[\"y_test\"], svm.predict(obj_train[\"x_test\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      6147\n",
      "           1       0.98      0.98      0.98       952\n",
      "\n",
      "    accuracy                           0.99      7099\n",
      "   macro avg       0.99      0.99      0.99      7099\n",
      "weighted avg       0.99      0.99      0.99      7099\n",
      "\n",
      "\n",
      "        ----------------------------------------------------------------------\n",
      "          True Negatives = 6129             |    False Positives = 18\n",
      "          False Negatives = 23            |    True Positives = 929\n",
      "        ----------------------------------------------------------------------\n",
      "            \n",
      "roc_auc_score = 0.9864560392238886\n",
      "avg_precision_score = 0.9605320495096242\n"
     ]
    }
   ],
   "source": [
    "logicReg = AlgoML.logisticRegression(obj_data=obj_train)\n",
    "\n",
    "Metrics.calculate_classification(obj_train[\"y_test\"], logicReg.predict(obj_train[\"x_test\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now predict\n",
    "y_test_pred = ml.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#And merge with the others null\n",
    "column_predicted = pd.DataFrame(inscribeds_column_cleaned.copy())\n",
    "column_predicted[target] = np.around(y_test_pred, 1)\n",
    "\n",
    "answer = pd.merge(pd.DataFrame(inscribeds_column_all), column_predicted,\n",
    "             on=\"NU_INSCRICAO\", how=\"outer\", right_index=True)\n",
    "\n",
    "answer.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To confirm if have tha same number null between answer and our columns cleaned\n",
    "_s031k = (inscribeds_column_all.shape)[0] - (inscribeds_column_cleaned.shape)[0]\n",
    "\n",
    "answer.isna().sum()[target] == _s031k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MERGE EXEMPLE\n",
    "inscribeds_column_all = _data_test[\"NU_INSCRICAO\"]\n",
    "\n",
    "inscribes_column_withoutn_null = _data_test.dropna(subset=features)\n",
    "pd.merge(inscribes_column_withoutn_null, pd.DataFrame(incribeds_column_all), how=\"outer\").shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
